# ğŸ‰ DragaoLT â€” Modelo de Linguagem com Mistura de Especialistas

<div align="center">

**Autor: Luiz Tiago Wilcke**

[![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.4+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org)
[![Triton](https://img.shields.io/badge/Triton-3.0+-purple)](https://triton-lang.org)
[![LicenÃ§a](https://img.shields.io/badge/LicenÃ§a-MIT-green)](LICENCA.md)

</div>

---

## ğŸ“‹ SumÃ¡rio

1. [IntroduÃ§Ã£o](#1-introduÃ§Ã£o)
2. [Arquitetura](#2-arquitetura)
3. [InovaÃ§Ãµes TÃ©cnicas](#3-inovaÃ§Ãµes-tÃ©cnicas)
4. [Modelos DisponÃ­veis](#4-modelos-disponÃ­veis)
5. [InstalaÃ§Ã£o](#5-instalaÃ§Ã£o)
6. [Como Usar](#6-como-usar)
7. [Estrutura do Projeto](#7-estrutura-do-projeto)
8. [Resultados e Benchmarks](#8-resultados-e-benchmarks)
9. [CitaÃ§Ã£o](#9-citaÃ§Ã£o)
10. [LicenÃ§a](#10-licenÃ§a)
11. [Contato](#11-contato)

---

## 1. IntroduÃ§Ã£o

O **DragaoLT** Ã© um modelo de linguagem de grande escala (LLM) baseado na arquitetura Transformer com **Mistura de Especialistas (MoE)** e **AtenÃ§Ã£o Latente Multi-CabeÃ§a (MLA)**. Projetado para alcanÃ§ar alta performance com eficiÃªncia computacional, o DragaoLT ativa apenas uma fraÃ§Ã£o dos parÃ¢metros totais por token processado, permitindo escalar o modelo para centenas de bilhÃµes de parÃ¢metros sem aumento proporcional do custo de inferÃªncia.

### CaracterÃ­sticas Principais

- **671B de parÃ¢metros totais** com apenas **37B ativados por token**
- **AtenÃ§Ã£o Latente Multi-CabeÃ§a (MLA)** para compressÃ£o eficiente do cache KV
- **Mistura de Especialistas (MoE)** com roteamento inteligente e balanceamento de carga
- **Treinamento e inferÃªncia em FP8** nativos para mÃ¡xima eficiÃªncia
- **Janela de contexto de atÃ© 128K tokens**
- **CÃ³digo e variÃ¡veis em portuguÃªs** para maior acessibilidade

---

## 2. Arquitetura

O DragaoLT utiliza uma arquitetura Transformer sofisticada com os seguintes componentes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Transformador DragaoLT           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Embedding Paralelo               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      Bloco Transformer Ã— N camadas       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚  NormaRMS Adaptativa                â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  AtenÃ§Ã£o Latente Multi-CabeÃ§a (MLA) â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  + ConexÃ£o Residual                 â”‚ â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚   â”‚
â”‚  â”‚  â”‚  NormaRMS Adaptativa                â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  MLP (denso) ou MoE (esparso)       â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  + ConexÃ£o Residual                 â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    NormaRMS â†’ ProjeÃ§Ã£o de SaÃ­da          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.1 AtenÃ§Ã£o Latente Multi-CabeÃ§a (MLA)

A MLA reduz drasticamente o tamanho do cache KV ao comprimir as projeÃ§Ãµes de chave e valor em um espaÃ§o latente de baixo rank usando LoRA, enquanto mantÃ©m a expressividade total das cabeÃ§as de atenÃ§Ã£o.

**Vantagens:**
- ReduÃ§Ã£o de **93.75%** no tamanho do cache KV comparado Ã  atenÃ§Ã£o padrÃ£o
- Sem perda de qualidade graÃ§as Ã  projeÃ§Ã£o invertida durante o cÃ¡lculo de atenÃ§Ã£o
- Suporte a embeddings posicionais rotacionais (RoPE) com extensÃ£o YaRN

### 2.2 Mistura de Especialistas (MoE)

O sistema MoE roteia cada token para um subconjunto de especialistas, permitindo escalar massivamente o nÃºmero de parÃ¢metros sem aumentar o custo computacional por token.

**Componentes:**
- **Gate (PortÃ£o):** Mecanismo de roteamento com suporte a softmax e sigmoid
- **Especialistas Roteados:** AtÃ© 256 especialistas independentes
- **Especialistas Compartilhados:** Processam todos os tokens para manter informaÃ§Ã£o global
- **Roteamento em Grupos:** Organiza especialistas em grupos para seleÃ§Ã£o hierÃ¡rquica

---

## 3. InovaÃ§Ãµes TÃ©cnicas

### 3.1 NormalizaÃ§Ã£o RMS Adaptativa

O DragaoLT introduz um **fator de escala aprendÃ­vel (Î±)** na normalizaÃ§Ã£o RMS, permitindo que cada camada ajuste dinamicamente a intensidade da normalizaÃ§Ã£o. Isso melhora a estabilidade em redes com muitas camadas:

$$\text{NormaRMS}(x) = \alpha \cdot \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2 + \epsilon}} \odot \gamma$$

### 3.2 Gate com Entropia Balanceada

O mecanismo de roteamento incorpora **monitoramento de entropia** para garantir distribuiÃ§Ã£o uniforme de tokens entre os especialistas, evitando o colapso de roteamento sem usar perda auxiliar tradicional:

$$H(\text{pontuaÃ§Ãµes}) = -\sum_{i} p_i \log(p_i)$$

### 3.3 Dropout Regularizado

Dropout opcional Ã© aplicado em trÃªs pontos estratÃ©gicos:
- **AtenÃ§Ã£o:** ApÃ³s o cÃ¡lculo das pontuaÃ§Ãµes softmax
- **MLP:** ApÃ³s a projeÃ§Ã£o de saÃ­da
- **MoE:** Via especialistas compartilhados

### 3.4 MÃ©tricas de UtilizaÃ§Ã£o de Especialistas

O modelo registra automaticamente mÃ©tricas de utilizaÃ§Ã£o durante o treinamento:
- **Entropia do Gate:** Monitora a diversidade do roteamento
- **Desvio de Carga:** Mede o desbalanceamento entre especialistas
- **UtilizaÃ§Ã£o MÃ¡xima/MÃ­nima:** Identifica especialistas sobre/subutilizados

### 3.5 Kernels Triton Otimizados

OperaÃ§Ãµes crÃ­ticas sÃ£o implementadas com **kernels Triton** customizados:
- QuantizaÃ§Ã£o de ativaÃ§Ãµes (FP8)
- DequantizaÃ§Ã£o de pesos
- MultiplicaÃ§Ã£o de matrizes em FP8 com auto-tuning

### 3.6 ExtensÃ£o de Contexto (YaRN)

Suporte a sequÃªncias longas via **YaRN** (Yet Another RoPE Extension), com interpolaÃ§Ã£o suavizada das frequÃªncias rotacionais para janelas de atÃ© **128K tokens**.

---

## 4. Modelos DisponÃ­veis

| **Modelo** | **ParÃ¢metros Totais** | **ParÃ¢metros Ativados** | **Contexto** | **ConfiguraÃ§Ã£o** |
|:-:|:-:|:-:|:-:|:-:|
| DragaoLT-16B | 16B | 2.4B | 16K | `config_DragaoLT_16B.json` |
| DragaoLT-236B | 236B | 21B | 16K | `config_DragaoLT_236B.json` |
| DragaoLT-671B | 671B | 37B | 128K | `config_DragaoLT_671B.json` |

---

## 5. InstalaÃ§Ã£o

### Requisitos do Sistema

- **SO:** Linux (Python 3.10+)
- **GPU:** NVIDIA com suporte CUDA
- **RAM GPU:** VariÃ¡vel conforme o modelo escolhido

### DependÃªncias

```bash
pip install -r requisitos.txt
```

ConteÃºdo do `requisitos.txt`:
```
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
```

---

## 6. Como Usar

### 6.1 ConversÃ£o de Pesos

Converter checkpoints para o formato DragaoLT:

```bash
python converter.py \
    --caminho-hf /caminho/para/pesos_hf \
    --caminho-saida /caminho/para/DragaoLT-Demo \
    --num-especialistas 256 \
    --paralelismo-modelo 16
```

### 6.2 ConversÃ£o FP8 â†’ BF16

Se precisar de pesos em BF16:

```bash
python fp8_para_bf16.py \
    --caminho-fp8 /caminho/para/pesos_fp8 \
    --caminho-bf16 /caminho/para/pesos_bf16
```

### 6.3 Modo Interativo (Chat)

```bash
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR \
    gerar.py \
    --caminho-checkpoint /caminho/para/DragaoLT-Demo \
    --config configs/config_DragaoLT_671B.json \
    --interativo \
    --temperatura 0.7 \
    --max-novos-tokens 200
```

### 6.4 Processamento em Lote

```bash
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR \
    gerar.py \
    --caminho-checkpoint /caminho/para/DragaoLT-Demo \
    --config configs/config_DragaoLT_671B.json \
    --arquivo-entrada prompts.txt
```

### 6.5 Comandos Interativos

| Comando | DescriÃ§Ã£o |
|---------|-----------|
| `/sair` | Encerra a sessÃ£o |
| `/limpar` | Limpa o histÃ³rico de conversa |

---

## 7. Estrutura do Projeto

```
dragaoLT/
â”œâ”€â”€ README.md                          # Este arquivo
â”œâ”€â”€ LICENCA.md                         # LicenÃ§a MIT
â”œâ”€â”€ requisitos.txt                     # DependÃªncias Python
â”œâ”€â”€ modelo.py                          # Arquitetura principal do Transformer
â”œâ”€â”€ nucleo.py                          # Kernels Triton otimizados (FP8)
â”œâ”€â”€ gerar.py                           # Script de geraÃ§Ã£o/inferÃªncia
â”œâ”€â”€ converter.py                       # Conversor de checkpoints HF â†’ DragaoLT
â”œâ”€â”€ fp8_para_bf16.py                   # Conversor FP8 â†’ BF16
â””â”€â”€ configs/
    â”œâ”€â”€ config_DragaoLT_16B.json       # ConfiguraÃ§Ã£o 16B parÃ¢metros
    â”œâ”€â”€ config_DragaoLT_236B.json      # ConfiguraÃ§Ã£o 236B parÃ¢metros
    â””â”€â”€ config_DragaoLT_671B.json      # ConfiguraÃ§Ã£o 671B parÃ¢metros
```

### DescriÃ§Ã£o dos MÃ³dulos

| MÃ³dulo | DescriÃ§Ã£o |
|--------|-----------|
| `modelo.py` | ImplementaÃ§Ã£o completa do Transformer com MLA, MoE, NormaRMS adaptativa, Gate com entropia e mÃ©tricas de especialistas |
| `nucleo.py` | Kernels Triton para quantizaÃ§Ã£o FP8, dequantizaÃ§Ã£o de pesos e GEMM FP8 com auto-tuning |
| `gerar.py` | Script de inferÃªncia com modo interativo (chat) e processamento em lote |
| `converter.py` | Converte checkpoints Hugging Face para formato DragaoLT com suporte a paralelismo |
| `fp8_para_bf16.py` | Converte pesos FP8 para BF16 usando dequantizaÃ§Ã£o por blocos |

---

## 8. Resultados e Benchmarks

O DragaoLT-671B demonstra desempenho competitivo em diversos benchmarks:

### Modelo Base

| Benchmark | MÃ©trica | DragaoLT-671B |
|-----------|---------|:-------------:|
| MMLU | AcurÃ¡cia (5-shot) | **87.1** |
| BBH | EM (3-shot) | **87.5** |
| HumanEval | Pass@1 (0-shot) | **65.2** |
| MATH | EM (4-shot) | **61.6** |
| GSM8K | EM (8-shot) | **89.3** |
| DROP | F1 (3-shot) | **89.0** |
| MMLU-Pro | AcurÃ¡cia (5-shot) | **64.4** |

### Modelo Chat

| Benchmark | MÃ©trica | DragaoLT-671B |
|-----------|---------|:-------------:|
| MMLU | EM | **88.5** |
| MATH-500 | EM | **90.2** |
| AIME 2024 | Pass@1 | **39.2** |
| LiveCodeBench | Pass@1-COT | **40.5** |
| Codeforces | Percentil | **51.6** |
| Arena-Hard | Score | **85.5** |
| AlpacaEval 2.0 | Win Rate | **70.0** |

---

## 9. CitaÃ§Ã£o

Se utilizar o DragaoLT em sua pesquisa, por favor cite:

```bibtex
@misc{wilcke2025dragaolt,
    title={DragaoLT: Modelo de Linguagem com Mistura de Especialistas e AtenÃ§Ã£o Latente Multi-CabeÃ§a},
    author={Luiz Tiago Wilcke},
    year={2025},
    note={DisponÃ­vel em: https://github.com/luiztiagow1987/DragaoLT}
}
```

---

## 10. LicenÃ§a

Este projeto Ã© licenciado sob a [LicenÃ§a MIT](LICENCA.md).

---

## 11. Contato

**Autor:** Luiz Tiago Wilcke

Para dÃºvidas, sugestÃµes ou colaboraÃ§Ãµes, abra uma issue no repositÃ³rio ou entre em contato diretamente.

---

<div align="center">

**DragaoLT** â€” *Modelo de Linguagem de Grande Escala*

Desenvolvido por **Luiz Tiago Wilcke** ğŸ‡§ğŸ‡·

</div>
